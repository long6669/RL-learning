# ====================================================
# First-Visit Monte Carlo Control with Exploring Starts
# ====================================================

import gymnasium as gym
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt

# åˆ›å»ºç¯å¢ƒ
env = gym.make('Blackjack-v1', render_mode=None)

# è¶…å‚æ•°
EPISODES = 500000
GAMMA = 1.0
MIN_EPSILON = 0.01
DECAY_RATE = 0.9999

# åˆå§‹åŒ–æ•°æ®ç»“æ„
# Qï¼šQ-Table
Q = defaultdict(lambda: np.zeros(env.action_space.n))

# N: å­˜å‚¨å‡ºç°æ¬¡æ•°ï¼Œè®¡ç®—å¹³å‡å€¼
N = defaultdict(lambda: np.zeros(env.action_space.n))

def get_action(state, epsilon):
    """Epsilon-Greedy ç­–ç•¥"""
    state_tuple = state
    if np.random.random() < epsilon:
        return env.action_space.sample()
    else:
        return np.argmax(Q[state_tuple])

# ====================================================
# è®­ç»ƒå¾ªç¯
# ====================================================
print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼šFirst-Visit Monte Carlo Control with {EPISODES} episodes...")

epsilon = 1.0

for episode in range(EPISODES):
    state, info = env.reset()
    state_tuple = state

    episode_history = []
    done = False

    while not done:
        action = get_action(state_tuple, epsilon)

        next_state, reward, terminated, truncated, info = env.step(action)

        done = terminated or truncated

        episode_history.append((state_tuple, action, reward))

        state_tuple = next_state
    # å›æŠ¥G
    G = 0
    # è®°å½•æœ¬å±€å·²ç»è®¿é—®è¿‡çš„ (çŠ¶æ€, åŠ¨ä½œ) å¯¹
    visited_state_actions = set()

    for t, (s, a, r) in enumerate(reversed(episode_history)):
        G = r + GAMMA * G

        if (s, a) not in visited_state_actions:
            visited_state_actions.add((s, a))

            N[s][a] += 1

            N_s_a = N[s][a]
            Q[s][a] += (G - Q[s][a]) / N_s_a

    epsilon = max(MIN_EPSILON, epsilon * DECAY_RATE)

    if (episode + 1) % 50000 == 0:
        print(f"Episode: {episode + 1}, Epsilon: {epsilon:.4f}")

print("âœ… è®­ç»ƒå®Œæˆï¼Q-Table å·²å¡«å……ã€‚")
env.close()

# ç°åœ¨ Q è¡¨é‡Œå­˜çš„å°±æ˜¯â€œæœ€ä¼˜ç­–ç•¥â€äº†ã€‚æˆ‘ä»¬æ¥è·‘ 1000 å±€çœ‹çœ‹èƒœç‡
env_test = gym.make('Blackjack-v1', render_mode=None)
wins = 0
tests = 10000

for _ in range(tests):
    state, _ = env_test.reset()
    done = False
    
    while not done:
        # å®Œå…¨è´ªå©ªç­–ç•¥ (epsilon=0)ï¼Œåªé€‰æ‹© Q è¡¨ä¸­æœ€å¥½çš„åŠ¨ä½œ
        action = np.argmax(Q[state])
        next_state, reward, terminated, truncated, _ = env_test.step(action)
        done = terminated or truncated
        state = next_state
        
    if reward == 1.0:
        wins += 1

env_test.close()
win_rate = wins / tests * 100

print(f"\n--- è¯„ä¼°ç»“æœ (MC Control) ---")
print(f"æµ‹è¯•å±€æ•°: {tests}")
# ä¸“å®¶çº§ç©å®¶çš„èƒœç‡é€šå¸¸åœ¨ 42% å·¦å³
print(f"AI èƒœç‡: {win_rate:.2f}% (é€šå¸¸åœ¨ 40% ~ 43% è§†ä¸ºæˆåŠŸ)")


    


